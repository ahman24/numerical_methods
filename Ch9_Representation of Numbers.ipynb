{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Chapter 9: Representation of Numbers\r\n",
    "by [Arief Rahman Hakim](https://github.com/ahman24)\r\n",
    "\r\n",
    "In this chapter, we will learn how numbers are representated in computer, how to convert between them and decimal numbers, their primary advantages and disadvantages and the roundoff errors.\r\n",
    "\r\n",
    "## 1. Base-N and Binary\r\n",
    "### 1.A Base-N\r\n",
    "The **decimal** system is a way of representing numbers that you are familiar with from elementary school. In the decimal system, a number is represented by a list of digits from 0 to 9, where **each digit represents the coefficient for a power of 10**.\r\n",
    "\r\n",
    "EXAMPLE: Show the decimal expansion for 147.3.\r\n",
    "\r\n",
    "$147.3 = 1⋅10^2 + 4⋅10^1 + 7⋅10^0 + 3⋅10^{− 1}$\r\n",
    "\r\n",
    "Since each digit is associated with a power of 10, the **decimal system is also known as base10** because it is based on 10 digits (0 to 9). However, there is nothing special about base10 numbers except perhaps that you are more accustomed to using them. \r\n",
    "\r\n",
    "For example, in **base3** we have the digits 0, 1, and 2 and the number 121 $(base 3)= 1⋅3^2 + 2⋅3^1 + 1⋅3^0 = 9 + 6 + 1 = 16(base 10)$\r\n",
    "\r\n",
    "### 1.B Binary\r\n",
    "A very important representation of numbers for computers is **base2 or binary numbers**. In binary, the only **available digits are 0 and 1**, and each digit is the coefficient of a power of 2. Digits in a binary number are also known as a bit.\r\n",
    "\r\n",
    "For instance,  \r\n",
    "$37 (base 10) = 32 + 4 + 1 = 1⋅2^5 + 0⋅2^4 + 0⋅2^3 + 1⋅2^2 + 0⋅2^1 + 1⋅2^0 = 100101 (base 2)$\r\n",
    "\r\n",
    "Unlike humans that can abstract numbers to arbitrarily large values, **computers have a fixed number of bits** that they are capable of storing at one time. For example, a **32-bit computer can represent and process 32-digit binary numbers** and no more. \r\n",
    "\r\n",
    "If all 32-bits are used to represent positive integer binary numbers, then this means that there are $\\sum_{n=0}^{31} 2^{n} = 4,294,967,296$ numbers the computer can represent. This **is not very many numbers at all** and would be completely insufficient to do any useful arithmetic on. For example, you could not compute the perfectly reasonable sum 0.5+1.25 using this representation because all the bits are dedicated to only integers."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 2. Floating Point Numbers\r\n",
    "Binary representation gives us an insufficient range and precision of numbers to do relevant engineering calculations. To achieve the range of values needed with the same number of bits, we use **floating point numbers** or `float` for short.\r\n",
    "\r\n",
    "**Instead of utilizing each bit as the coefficient of a power of 2**, floats allocate bits to three different parts: \r\n",
    "* the sign indicator, s, which says whether a number is positive or negative; \r\n",
    "* characteristic or exponent, $e$, which is the power of 2; \r\n",
    "* fraction, `f`, which is the coefficient of the exponent. \r\n",
    "\r\n",
    "\r\n",
    "<img src=\"img/09.02.01-Binary_neg_12.png\" alt=\"Float in Binary System\" width=\"400\"/>\r\n",
    "\r\n",
    "\r\n",
    "Almost all platforms map Python floats to the **IEEE754 double precision - 64 total bits**. **1 bit** is allocated to the **sign indicator**, **11 bits** are allocated to the **exponent**, and **52 bits** are allocated to the **fraction**. With 11 bits allocated to the exponent, this makes **2048 values that this number can take**. \r\n",
    "\r\n",
    "Since we want to be able to make very precise numbers, we want some of these values to represent negative exponents (i.e., to allow numbers that are between 0 and 1 (base10)). To accomplish this, 1023 is subtracted from the exponent to normalize it. The value subtracted from the exponent is commonly referred to as the bias. The fraction is a number between 1 and 2. In binary, this means that the leading term will always be 1, and, therefore, it is a waste of bits to store it. To save space, the leading 1 is dropped. In Python, we could get the float information using the sys package as shown below:"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "source": [
    "import sys\r\n",
    "sys.float_info"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "sys.float_info(max=1.7976931348623157e+308, max_exp=1024, max_10_exp=308, min=2.2250738585072014e-308, min_exp=-1021, min_10_exp=-307, dig=15, mant_dig=53, epsilon=2.220446049250313e-16, radix=2, rounds=1)"
      ]
     },
     "metadata": {},
     "execution_count": 1
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "following conversion: `15 (base10)` = 0 10000000010 1110000000000000000000000000000000000000000000000000 (IEEE754)\r\n",
    "\r\n",
    "\r\n",
    "The next smallest number is 0 10000000010 1101111111111111111111111111111111111111111111111111 = **14.9999999999999982236431605997**\r\n",
    "\r\n",
    "The next largest number is 0 10000000010 1110000000000000000000000000000000000000000000000001 = **15.0000000000000017763568394003**  \r\n",
    "\r\n",
    "\r\n",
    "Therefore, the IEEE754 number **not only represents the number 15.0, but also all the real numbers halfway between its immediate neighbors. So any computation that has a result within this interval will be assigned 15.0.**\r\n",
    "\r\n",
    "Moreover,"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "source": [
    "import numpy as np\r\n",
    "\r\n",
    "largest = (2**(2046-1023))*((1 + sum(0.5**np.arange(1, 53))))\r\n",
    "largest"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "1.7976931348623157e+308"
      ]
     },
     "metadata": {},
     "execution_count": 3
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "source": [
    "sys.float_info.max"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "1.7976931348623157e+308"
      ]
     },
     "metadata": {},
     "execution_count": 4
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "source": [
    "smallest = (2**(1-1023))*(1+0)\r\n",
    "smallest"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "2.2250738585072014e-308"
      ]
     },
     "metadata": {},
     "execution_count": 5
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "source": [
    "sys.float_info.min"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "2.2250738585072014e-308"
      ]
     },
     "metadata": {},
     "execution_count": 6
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Numbers that are larger than the largest representable floating point number result in **overflow**, and Python handles this case by assigning the result to **inf**. Numbers that are smaller than the smallest subnormal number result in **underflow**, and Python handles this case by assigning the result to **0**.\r\n",
    "\r\n",
    "So, what have we gained by using IEEE754 versus binary?  \r\n",
    "Using 64 bits binary gives us 264 numbers. Since the number of bits does not change between binary and IEEE754, IEEE754 must also give us 264 numbers. In **binary**, **numbers have a constant spacing between them**. As a result, you **cannot have both range** (i.e., large distance between minimum and maximum representable numbers) and **precision** (i.e., small spacing between numbers). Controlling these parameters would depend on where you put the decimal point in your number. \r\n",
    "\r\n",
    "**IEEE754** overcomes this limitation by **using very high precision at small numbers** and **very low precision at large numbers**. This limitation is usually acceptable because the gap at large numbers is still small relative to the size of the number itself. Therefore, even if the gap is millions large, it is irrelevant to normal calculations if the number under consideration is in the trillions or higher."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 3. Round-offs Errors"
   ],
   "metadata": {}
  }
 ],
 "metadata": {
  "orig_nbformat": 4,
  "language_info": {
   "name": "python",
   "version": "3.7.10",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "file_extension": ".py"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.7.10 64-bit ('base': conda)"
  },
  "interpreter": {
   "hash": "07efdcd4b820c98a756949507a4d29d7862823915ec7477944641bea022f4f62"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}