{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Chapter 11: Reading and Writing Data\r\n",
    "by [Arief Rahman Hakim](https://github.com/ahman24)"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "source": [
    "# Path to current working dir\r\n",
    "import os\r\n",
    "\r\n",
    "output_dir = os.path.join(os.getcwd(), 'output')\r\n",
    "print(output_dir)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "d:\\Python\\Numerical Methods\\numerical_methods\\output\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 1. TXT files\r\n",
    "To work with text files, we need to use open function which returns a file object. It is commonly used with two arguments:\r\n",
    "\r\n",
    "> f = open(filename, mode)  \r\n",
    "\r\n",
    "The modes are,\r\n",
    "* `r`: this is the default mode, which opens a file for reading\r\n",
    "* `w`: this mode opens a file for writing, if the file does not exist, it creates a new file.\r\n",
    "* `a`: open a file in append mode, append data to end of file. If the file does not exist, it creates a new file.\r\n",
    "* `b`: open a file in binary mode.\r\n",
    "* `r+`: open a file (do not create) for reading and writing.\r\n",
    "* `w+`: open or create a file for writing and reading, discard existing contents.\r\n",
    "* `a+`: open or create file for reading and writing, and append data to end of file."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "There are 2 ways of writing a txt file,\r\n",
    "* 'typical' approach\r\n",
    "* context manager\r\n",
    "\r\n",
    "Let's see how those compared,"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "source": [
    "# Define the intended path to the file\r\n",
    "name_file = 'Ch11_txt_typical_approach.txt'\r\n",
    "path_file = os.path.join(output_dir, name_file)\r\n",
    "\r\n",
    "# Typical approach\r\n",
    "f = open(path_file, 'w')\r\n",
    "for i in range(5):\r\n",
    "    f.write(f'This is line{i}')\r\n",
    "f.close()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "source": [
    "# Define the intended path to the file\r\n",
    "name_file = 'Ch11_txt_context_manager.txt'\r\n",
    "path_file = os.path.join(output_dir, name_file)\r\n",
    "\r\n",
    "with open(path_file, 'w') as f:\r\n",
    "    for i in range(5):\r\n",
    "        f.write(f\"This is line {i}\\n\")"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "If you notice, the context manager `with` does not require to close the file manually. The context manager will do the job. The object `f` will be cleaned automatically. If possible, proceed with context manager to open, read, etc files."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Sometimes, all we need is to append a line to the file. We could do just that as follows,"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "source": [
    "with open(path_file, 'a') as f:\r\n",
    "    f.write(f\"This is another line\\n\")"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "We could read the file as follows,"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "source": [
    "with open(path_file, 'r') as f:\r\n",
    "    content = f.read()\r\n",
    "\r\n",
    "print(content)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "This is line 0\n",
      "This is line 1\n",
      "This is line 2\n",
      "This is line 3\n",
      "This is line 4\n",
      "This is another line\n",
      "\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Since mostlikely we will be working with `arrays` on `numpy`, we could also save the file directly with the built in feature in numpy as follows,"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "source": [
    "import numpy as np\r\n",
    "arr = np.array([[1.20, 2.20, 3.00], [4.14, 5.65, 6.42]])\r\n",
    "arr"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "array([[1.2 , 2.2 , 3.  ],\n",
       "       [4.14, 5.65, 6.42]])"
      ]
     },
     "metadata": {},
     "execution_count": 6
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "source": [
    "np.savetxt(path_file, arr, fmt='%.2f', header = 'Col1 Col2 Col3')"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "source": [
    "with open(path_file, 'r') as f:\r\n",
    "    content = f.read()\r\n",
    "\r\n",
    "print(content)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "# Col1 Col2 Col3\n",
      "1.20 2.20 3.00\n",
      "4.14 5.65 6.42\n",
      "\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Above we save the file with only **2 significant digits**,  \r\n",
    "Let's load the file again with `numpy`,"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "source": [
    "my_arr = np.loadtxt(path_file)\r\n",
    "my_arr"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "array([[1.2 , 2.2 , 3.  ],\n",
       "       [4.14, 5.65, 6.42]])"
      ]
     },
     "metadata": {},
     "execution_count": 9
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 2. CSV files\r\n",
    "There are many scientific data are stored in the **comma-separated values (CSV)** file format, a delimited text file that uses a comma to separate values. It is a very useful format that can store large tables of data (numbers and text) in plain text.\r\n",
    "\r\n",
    "Let's see how it works,"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "source": [
    "# Define the intended path to the file\r\n",
    "name_file = 'Ch11_csv.csv'\r\n",
    "path_file = os.path.join(output_dir, name_file)\r\n",
    "\r\n",
    "data = np.random.random((100,5))\r\n",
    "np.savetxt(path_file, data, fmt = '%.2f', delimiter=',', header = 'c1, c2, c3, c4, c5')"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Let's load the array with `numpy` again,"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "source": [
    "my_csv = np.loadtxt(path_file, delimiter=',')\r\n",
    "my_csv[:5, :]"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "array([[0.55, 0.22, 0.12, 0.35, 0.44],\n",
       "       [0.53, 0.79, 0.89, 0.63, 0.87],\n",
       "       [0.27, 0.14, 0.02, 0.36, 0.66],\n",
       "       [0.51, 0.35, 0.9 , 0.34, 0.85],\n",
       "       [0.53, 0.05, 0.76, 0.9 , 0.47]])"
      ]
     },
     "metadata": {},
     "execution_count": 11
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 3. Pickle files\r\n",
    "We might want to store `dictionaries`, `tuples`, `lists`, or any other data type to the disk and use them later or send them to some colleagues. **Pickle** can serialize objects so that they can be saved into a file and loaded again later.\r\n",
    "\r\n",
    "Pickle can be used to serialize Python object structures, which refers to the process of **converting an object in the memory to a byte stream that can be stored as a binary file on disk**. When we load it back to a Python program, this binary file can be de-serialized back to a Python object."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "source": [
    "# Define the intended path to the file\r\n",
    "name_file = 'Ch11_pickle.pkl'\r\n",
    "path_file = os.path.join(output_dir, name_file)\r\n",
    "\r\n",
    "import pickle\r\n",
    "dict_a = {'A':0, 'B':1, 'C':2}\r\n",
    "pickle.dump(dict_a, open(path_file, 'wb'))"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "We could load the pickle file again as follows,"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "source": [
    "my_dict = pickle.load(open(path_file, 'rb'))\r\n",
    "my_dict"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "{'A': 0, 'B': 1, 'C': 2}"
      ]
     },
     "metadata": {},
     "execution_count": 13
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 4. JSON files\r\n",
    "JSON stands for **JavaScript Object Notation**. Unlike pickle, which is Python dependent, JSON is a **language-independent data format**. Besides, it is usually takes **less space on the disk** and the **manipulation is faster** than pickle (if you are interested, search online to find more materials about it).\r\n",
    "\r\n",
    "Let's take a look on the JSON format,"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "source": [
    "school = {\r\n",
    "  \"school\": \"UC Berkeley\",\r\n",
    "  \"address\": {\r\n",
    "    \"city\": \"Berkeley\", \r\n",
    "    \"state\": \"California\",\r\n",
    "    \"postal\": \"94720\"\r\n",
    "  }, \r\n",
    "    \r\n",
    "  \"list\":[\r\n",
    "      \"student 1\",\r\n",
    "      \"student 2\",\r\n",
    "      \"student 3\"\r\n",
    "      ]\r\n",
    "}\r\n",
    "school"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "{'school': 'UC Berkeley',\n",
       " 'address': {'city': 'Berkeley', 'state': 'California', 'postal': '94720'},\n",
       " 'list': ['student 1', 'student 2', 'student 3']}"
      ]
     },
     "metadata": {},
     "execution_count": 14
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "We could write a JSON file with the `json` library,"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "source": [
    "import json\r\n",
    "# Define the intended path to the file\r\n",
    "name_file = 'Ch11_json.json'\r\n",
    "path_file = os.path.join(output_dir, name_file)\r\n",
    "\r\n",
    "json.dump(school, open(path_file, 'w'))"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "We could load the json file as follows,"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "source": [
    "my_school = json.load(open(path_file, 'r'))\r\n",
    "my_school"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "{'school': 'UC Berkeley',\n",
       " 'address': {'city': 'Berkeley', 'state': 'California', 'postal': '94720'},\n",
       " 'list': ['student 1', 'student 2', 'student 3']}"
      ]
     },
     "metadata": {},
     "execution_count": 16
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 5. HDF5 files\r\n",
    "HDF5 stands for **Hierarchical Data Format**. HDF5 helps to store **large amounts of data with quick access**. HDF5 is a powerful binary data format with no limit on the file size. It **provides parallel IO (input/output)**, and carries out a **bunch of low level optimizations** under the hood to make the **queries faster and storage requirements smaller**.\r\n",
    "\r\n",
    "An HDF5 file saves two types of objects: \r\n",
    "* datasets: `array-like` collections of data (like NumPy arrays), \r\n",
    "* groups: `folder-like` containers that hold datasets and other groups. \r\n",
    "\r\n",
    "There are also attributes that could associate with the datasets and groups to describe some properties. The so called hierarchical in HDF5 refers to the fact that **the data could be saved like a file system, with folder-like structures, such as folder, subfolder (in HDF5, it is called group, subgroup)**. Groups operate like dictionaries with the `keys` and `values`, with the **keys are names of the groups**, and the **values are the subgroups or datasets**."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Let's demonstrate the HDF5 below,\r\n",
    "\r\n",
    "**Example**  \r\n",
    "Suppose we deployed some instruments to monitor the accelerations and GPS location in Bay Area, CA. We deployed two accelerometers at Berkeley and Oakland as well as one GPS station at San Fransisco. And they record data at different sampling rates, with the accelerometer at Berkeley sample the data every 0.04 s, and 0.01 s for the sensor at Oakland. The GPS samples the location every 60 seconds in San Fransisco. Now we want to store the two types of data into a HDF5 as well as some attributes indicate where the data is recorded, start time of the recording, station name and the sampling interval."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "source": [
    "# Generate random data for recording\r\n",
    "acc_1 = np.random.random(1000)\r\n",
    "station_number_1 = '1'\r\n",
    "# unix timestamp\r\n",
    "start_time_1 = 1542000276\r\n",
    "# time interval for recording\r\n",
    "dt_1 = 0.04\r\n",
    "location_1 = 'Berkeley'\r\n",
    "\r\n",
    "acc_2 = np.random.random(500)\r\n",
    "station_number_2 = '2'\r\n",
    "start_time_2 = 1542000576\r\n",
    "dt_2 = 0.01\r\n",
    "location_2 = 'Oakland'"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "source": [
    "# Define the intended path to the file\r\n",
    "name_file = 'Ch11_hdf5.hdf5'\r\n",
    "path_file = os.path.join(output_dir, name_file)\r\n",
    "\r\n",
    "import h5py\r\n",
    "\r\n",
    "# Let's use the context manager\r\n",
    "with h5py.File(path_file, 'w') as hf:\r\n",
    "    hf['/acc/1/data'] = acc_1\r\n",
    "    hf['/acc/1/data'].attrs['dt'] = dt_1\r\n",
    "    hf['/acc/1/data'].attrs['start_time'] = start_time_1\r\n",
    "    hf['/acc/1/data'].attrs['location'] = location_1\r\n",
    "\r\n",
    "    hf['/acc/2/data'] = acc_2\r\n",
    "    hf['/acc/2/data'].attrs['dt'] = dt_2\r\n",
    "    hf['/acc/2/data'].attrs['start_time'] = start_time_2\r\n",
    "    hf['/acc/2/data'].attrs['location'] = location_2\r\n",
    "\r\n",
    "    hf['/gps/1/data'] = np.random.random(100)\r\n",
    "    hf['/gps/1/data'].attrs['dt'] = 60\r\n",
    "    hf['/gps/1/data'].attrs['start_time'] = 1542000000\r\n",
    "    hf['/gps/1/data'].attrs['location'] = 'San Francisco'"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "We can see we have two top level groups,\r\n",
    "* acc,\r\n",
    "* gps, \r\n",
    "\r\n",
    "both of them contains **subgroups 1 or 2** indicate the station names. **Each station will contain the next level subgroup**, `data`, that is used to store the array data we created. We could then **add attributes to the groups or the data**. Here we only added the `dt`, `start_time`, and `location` as the attributes to the datasets we store here. You can see that it is quite similar to folder-like structure, with data `acc_1` saved at `/acc/1/data`.\r\n",
    "\r\n",
    "We could load the HDF5 file again as follows,"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "source": [
    "with h5py.File(path_file, 'r') as hf:\r\n",
    "    hf = h5py.File(path_file, 'r')"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "source": [
    "list(hf.keys())"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "['acc', 'gps']"
      ]
     },
     "metadata": {},
     "execution_count": 20
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "source": [
    "acc = hf['acc']\r\n",
    "list(acc.keys())"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "['1', '2']"
      ]
     },
     "metadata": {},
     "execution_count": 21
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "source": [
    "data_1 = hf['acc/1/data']\r\n",
    "data_1[:10]"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "array([0.04102403, 0.69825357, 0.42450819, 0.34359535, 0.43676345,\n",
       "       0.22934737, 0.28688871, 0.22603361, 0.60105346, 0.16275696])"
      ]
     },
     "metadata": {},
     "execution_count": 22
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "source": [
    "print(list(data_1.attrs))\r\n",
    "print(data_1.attrs['dt'])\r\n",
    "print(data_1.attrs['location'])\r\n",
    "print(data_1.attrs['start_time'])"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "['dt', 'location', 'start_time']\n",
      "0.04\n",
      "Berkeley\n",
      "1542000276\n"
     ]
    }
   ],
   "metadata": {}
  }
 ],
 "metadata": {
  "orig_nbformat": 4,
  "language_info": {
   "name": "python",
   "version": "3.7.10",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "file_extension": ".py"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.7.10 64-bit ('base': conda)"
  },
  "interpreter": {
   "hash": "07efdcd4b820c98a756949507a4d29d7862823915ec7477944641bea022f4f62"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}